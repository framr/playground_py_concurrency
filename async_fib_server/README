Reproducing stuff from D.Beazley talk "Concurrency from the ground up" (a great one!) 

There are few servers here
- threading server handles each client connection in a separate thread
- process pool server - again handles each client in a separate thread, but in addition it performs the calculations using process pool 
(although in a blocking way)

To try server you may type in:
nc localhost 25000
30


In addition there are two scripts for performance testing
perf1.py measures time required to execute computation heavy request, fib(30)
perf2.py measures rps for light requests, fib(1)


1. Threading server. Clients handled in separate threads.
When only one client present, threading server is able to handle very high rps-s.
However, if there is another client rps drops by enormous factor. This is because 
a) GIL
b) when passing control to another thread Python prioritizes heavy computations (this is opposite to what os is normally doing)
58-60 krps, drops to 10 krps when both perf scripts are running

2. Threading + ProcessPool server - clients handled in separate threads, each request is processed by pool of processes (well, in a blocking way)  
In case of process pool server rps for one clients is way lower because of overhead for spawning process.
But it is much more stable in case of two cliens, because the computations are handled by different processes.
3.3 krps, minor drop under parallel heavy computations.

3. Vanilla process server. Each client is handled in a new process.
This is actually the fastest server, when measured by our performance scripts (because we have only two clients, process creation overhead is low
and we enjoy alse pure process parallelism).
57 krps for light requests, almost no performance drop under parallel heavy computations.
 
 

